{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = pd.read_csv('train_bigger.csv', dtype={\"from_2\": object, \"type\": object, \"category\": object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled.dropna(subset=['category'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled = pd.read_csv('labeled_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled.drop(columns=['text_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled['text_2']=labeled['from_2'].astype(str) + \" \" + labeled['text'].astype(str)\n",
    "unlabeled['text_2']=unlabeled['from_2'].astype(str) + \" \" + unlabeled['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['conv' 'end' 'mcq' 'non mcq' 'setup' 'start']\n",
      "Number of unique words: 492\n",
      "Average length: 45.246688741721854\n",
      "max length: 821\n",
      "Standard Deviation: 74.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Categories:\", np.unique(labeled['category']))\n",
    "print(\"Number of unique words:\", len(np.unique(np.hstack(labeled['text']))))\n",
    "\n",
    "length = [len(i) for i in labeled['text']]\n",
    "print(\"Average length:\", np.mean(length))\n",
    "print(\"max length:\", np.max(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=labeled['category']\n",
    "unlabeled['category']=np.nan\n",
    "val_labels=unlabeled['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=labeled['text_2']\n",
    "val_docs=unlabeled['text_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "labels=le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604,) (1084520,)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape, val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes=6\n",
    "# labels = keras.utils.to_categorical(labels,num_classes)\n",
    "# val_labels = keras.utils.to_categorical(val_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=492, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',)\n",
    "tokenizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "val_docs = tokenizer.texts_to_sequences(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = sequence.pad_sequences(docs, maxlen=821)\n",
    "val_docs = sequence.pad_sequences(val_docs, maxlen=821)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(604, 821)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, labels, random_state =42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Embedding\n",
    "# from keras.layers import Flatten\n",
    "\n",
    "# model=Sequential()\n",
    "# model.add(Embedding(input_dim=492,\n",
    "#                     output_dim=128,\n",
    "#                     input_length=821))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(604, activation='relu'))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam',metrics=['acc'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train, y_train, epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, accuracy = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test accuracy:0.77685950216183'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(f'test loss:{loss} \\n test accuracy:{accuracy}')\n",
    "f'test loss:0.7352383644127649' \n",
    "f'test accuracy:0.77685950216183'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV as RS\n",
    "from scipy import stats\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def create_model(dropout_rate=0.0, weight_constraint=0, hidden_layers=1, neurons=1, input_neurons=1, dropout_layers=1, embedding=1):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim=492, \n",
    "                       output_dim=embedding, \n",
    "                       input_length=821))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(input_neurons, activation='relu'))\n",
    "    for i in range(hidden_layers, dropout_layers):\n",
    "        model.add(Dense(neurons, activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'input_neurons': stats.randint(1,128),\n",
    "        'neurons': stats.randint(1,128),\n",
    "        'hidden_layers': stats.randint(1,16),\n",
    "        'dropout_layers': stats.randint(1,16),\n",
    "        'dropout_rate': stats.uniform(0,0.9),\n",
    "        'weight_constraint': stats.uniform(1,5),\n",
    "        'embedding': stats.randint(1,256)\n",
    "       }\n",
    "n_iter=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KerasClassifier(build_fn=create_model, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rand = RS(estimator=model, param_distributions=params, n_jobs=-1, cv=4, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rand_search = rand.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores- from kaggle:https://www.kaggle.com/ksjpswaroop/parameter-tuning-rf-randomized-search\n",
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report(rand_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2(dropout_rate=0.216, weight_constraint=1.1489, hidden_layers=11, neurons=64, input_neurons=98, dropout_layers=5, embedding=168, batch_size=1, epochs=1):\n",
    "    model2=Sequential()\n",
    "    model2.add(Embedding(input_dim=492, \n",
    "                       output_dim=embedding, \n",
    "                       input_length=821))\n",
    "    model2.add(Flatten())\n",
    "    model2.add(Dense(input_neurons, activation='relu'))\n",
    "    for i in range(hidden_layers, dropout_layers):\n",
    "        model2.add(Dense(neurons, activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model2.add(Dropout(dropout_rate))\n",
    "    model2.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    model2.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2={'batch_size': stats.randint(1,128),\n",
    "         'epochs': stats.randint(1,64)}\n",
    "n_iter=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = KerasClassifier(build_fn=create_model_2, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand2 = RS(estimator=model2, param_distributions=params2, n_jobs=-1, cv=4, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search2 = rand2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(rand_search2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_3(dropout_rate=0.0747, weight_constraint=5.926, hidden_layers=15, neurons=1, input_neurons=31, dropout_layers=12, embedding=19):\n",
    "    model3=Sequential()\n",
    "    model3.add(Embedding(input_dim=492, \n",
    "                       output_dim=embedding, \n",
    "                       input_length=821))\n",
    "    model3.add(Flatten())\n",
    "    model3.add(Dense(input_neurons, activation='relu'))\n",
    "    for i in range(hidden_layers, dropout_layers):\n",
    "        model3.add(Dense(neurons, activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model3.add(Dropout(dropout_rate))\n",
    "    model3.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    model3.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = KerasClassifier(build_fn=create_model_3, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=54, \n",
    "            epochs=62,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_test, y_test),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history.history\n",
    "print('Validation accuracy: {acc}, loss: {loss}'.format(acc=history['val_acc'][-1], loss=history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
