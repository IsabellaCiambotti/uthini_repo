{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using a neural network to classify message categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identified message categories in generated training set:\n",
    "- MCQ: multiple choice question or answer\n",
    "- NONMCQ: content question or answer\n",
    "- CONV: small talk/pleasantries, or any non content-related message. \n",
    "- START: session start marker\n",
    "- END: session end marker\n",
    "- SETUP: sent as part of setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*using keras with TensorFlow as backend recommended*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and prepare in datasets: generated train data, and entire set of unlabeled messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = pd.read_csv('train_bigger.csv', dtype={\"from_2\": object, \"type\": object, \"category\": object})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to time constraints, we didn't generate a category for the entire fraction of rows randomly sampled from the full dataframe. This means we need to drop all the uncategorized data from our \"labeled\" training set to make it an actually labeled training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled.dropna(subset=['category'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled = pd.read_csv('labeled_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just good practice when classifying\n",
    "unlabeled.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this column was used/generated for previous text analysis; we don't need it here\n",
    "unlabeled.drop(columns=['text_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we add one of our generated attributes to the text data in the hopes of improving the classifier's accuracy\n",
    "\n",
    "once our other NN is run on all the data, it would be good to include that generated column, type, in the same way here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled['text_2']=unlabeled['from_2'].astype(str) + \" \" + unlabeled['text'].astype(str)\n",
    "labeled['text_2']=labeled['from_2'].astype(str) + \" \" + labeled['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['conv' 'end' 'mcq' 'non mcq' 'setup' 'start']\n",
      "Number of unique words: 492\n",
      "Average length: 45.246688741721854\n",
      "max length: 821\n",
      "Standard Deviation: 74.0\n"
     ]
    }
   ],
   "source": [
    "# just getting basic information about the text data \n",
    "print(\"Categories:\", np.unique(labeled['category']))\n",
    "print(\"Number of unique words:\", len(np.unique(np.hstack(labeled['text']))))\n",
    "\n",
    "length = [len(i) for i in labeled['text']]\n",
    "print(\"Average length:\", np.mean(length))\n",
    "print(\"max length:\", np.max(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish labeled and validation sets\n",
    "labels=labeled['category']\n",
    "unlabeled['category']=np.nan\n",
    "val_labels=unlabeled['category']\n",
    "docs=labeled['text_2']\n",
    "val_docs=unlabeled['text_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604,) (1084520,)\n"
     ]
    }
   ],
   "source": [
    "# Label encode the data so our categories are readable by a NN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le=LabelEncoder()\n",
    "labels=le.fit_transform(labels)\n",
    "print(labels.shape, val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional one-hot encoding of labels. Doesn't seem to improve accuracy one way or another at the moment\n",
    "#num_classes=6\n",
    "# labels = keras.utils.to_categorical(labels,num_classes)\n",
    "# val_labels = keras.utils.to_categorical(val_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text data so we can use an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use keras' text processing modules to create a \"vocabulary\" for our dataset \n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "\n",
    "unique_words=len(np.unique(np.hstack(labeled['text'])))\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=unique_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',)\n",
    "tokenizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokenized texts to sequence vectors\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "val_docs = tokenizer.texts_to_sequences(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequence vectors so they're all the same length (necessary for text processing)\n",
    "docs = sequence.pad_sequences(docs, maxlen=821)\n",
    "val_docs = sequence.pad_sequences(val_docs, maxlen=821)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604, 821) (1084520, 821)\n"
     ]
    }
   ],
   "source": [
    "print(docs.shape, val_docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our labeled dataset into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, labels, random_state =42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing our first neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*using a random selection of hyperparameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some parameters\n",
    "\n",
    "input_dim= unique_words\n",
    "length = [len(i) for i in labeled['text']]\n",
    "input_length= np.max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=492,\n",
    "                    output_dim=128,\n",
    "                    input_length=821))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(604, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam',metrics=['acc'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'test loss:{loss} \\n test accuracy:{accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm, that accuracy is way too high. We're not sure why that is....?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using sklearn's Keras Classifier wrapper and scipy stats to perform a random search\n",
    "\n",
    "you have to run this on an instance for it to be at all computeable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### round one hyperparameters:\n",
    "- hidden layers\n",
    "- neurons\n",
    "- input neurons\n",
    "- dropout layers\n",
    "- dropout rate\n",
    "- weight constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV as RS\n",
    "from scipy import stats\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dropout_rate=0.0, weight_constraint=0, hidden_layers=1, neurons=1, input_neurons=1, dropout_layers=1, embedding=1):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim=492, \n",
    "                       output_dim=128, \n",
    "                       input_length=821))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(input_neurons, activation='relu'))\n",
    "    for i in range(hidden_layers, dropout_layers):\n",
    "        model.add(Dense(neurons, activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'input_neurons': stats.randint(1,128),\n",
    "        'neurons': stats.randint(1,128),\n",
    "        'hidden_layers': stats.randint(1,16),\n",
    "        'dropout_layers': stats.randint(1,16),\n",
    "        'dropout_rate': stats.uniform(0,0.9),\n",
    "        'weight_constraint': stats.randint(1,5),\n",
    "       }\n",
    "n_iter=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = RS(estimator=model, param_distributions=params, n_jobs=-1, cv=4, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search = rand.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is a utility function to report best scores \n",
    "  *from kaggle:https://www.kaggle.com/ksjpswaroop/parameter-tuning-rf-randomized-search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(rand_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using round one hyperparameters to search for batch size and epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2(dropout_rate=0.216, weight_constraint=1.1489, hidden_layers=11, neurons=64, input_neurons=98, dropout_layers=5, embedding=168, batch_size=1, epochs=1):\n",
    "    model2=Sequential()\n",
    "    model2.add(Embedding(input_dim=492, \n",
    "                       output_dim=embedding, \n",
    "                       input_length=821))\n",
    "    model2.add(Flatten())\n",
    "    model2.add(Dense(input_neurons, activation='relu'))\n",
    "    for i in range(hidden_layers, dropout_layers):\n",
    "        model2.add(Dense(neurons, activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model2.add(Dropout(dropout_rate))\n",
    "    model2.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    model2.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2={'batch_size': stats.randint(1,128),\n",
    "         'epochs': stats.randint(1,64)}\n",
    "n_iter=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = KerasClassifier(build_fn=create_model_2, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand2 = RS(estimator=model2, param_distributions=params2, n_jobs=-1, cv=4, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search2 = rand2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(rand_search2.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a final model with best-performing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_3(dropout_rate=0.0747, weight_constraint=5.926, hidden_layers=15, neurons=1, input_neurons=31, dropout_layers=12, embedding=19):\n",
    "    model3=Sequential()\n",
    "    model3.add(Embedding(input_dim=492, \n",
    "                       output_dim=embedding, \n",
    "                       input_length=821))\n",
    "    model3.add(Flatten())\n",
    "    model3.add(Dense(input_neurons, activation='relu'))\n",
    "    for i in range(hidden_layers, dropout_layers):\n",
    "        model3.add(Dense(neurons, activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model3.add(Dropout(dropout_rate))\n",
    "    model3.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    model3.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = KerasClassifier(build_fn=create_model_3, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 483 samples, validate on 121 samples\n",
      "Epoch 1/62\n",
      " - 0s - loss: 1.5718 - acc: 0.4306 - val_loss: 1.3140 - val_acc: 0.5537\n",
      "Epoch 2/62\n",
      " - 0s - loss: 1.4222 - acc: 0.4928 - val_loss: 1.3088 - val_acc: 0.5537\n",
      "Epoch 3/62\n",
      " - 0s - loss: 1.3783 - acc: 0.4928 - val_loss: 1.2824 - val_acc: 0.5537\n",
      "Epoch 4/62\n",
      " - 0s - loss: 1.3619 - acc: 0.4928 - val_loss: 1.2581 - val_acc: 0.5537\n",
      "Epoch 5/62\n",
      " - 0s - loss: 1.3716 - acc: 0.4928 - val_loss: 1.2432 - val_acc: 0.5537\n",
      "Epoch 6/62\n",
      " - 0s - loss: 1.3603 - acc: 0.4969 - val_loss: 1.2582 - val_acc: 0.5537\n",
      "Epoch 7/62\n",
      " - 0s - loss: 1.3324 - acc: 0.4928 - val_loss: 1.2278 - val_acc: 0.5537\n",
      "Epoch 8/62\n",
      " - 0s - loss: 1.3120 - acc: 0.4948 - val_loss: 1.2054 - val_acc: 0.5537\n",
      "Epoch 9/62\n",
      " - 0s - loss: 1.2931 - acc: 0.5135 - val_loss: 1.1888 - val_acc: 0.5702\n",
      "Epoch 10/62\n",
      " - 0s - loss: 1.2667 - acc: 0.5466 - val_loss: 1.1604 - val_acc: 0.6116\n",
      "Epoch 11/62\n",
      " - 0s - loss: 1.2342 - acc: 0.5528 - val_loss: 1.1334 - val_acc: 0.6281\n",
      "Epoch 12/62\n",
      " - 0s - loss: 1.2083 - acc: 0.5818 - val_loss: 1.1106 - val_acc: 0.6529\n",
      "Epoch 13/62\n",
      " - 0s - loss: 1.1816 - acc: 0.5859 - val_loss: 1.0844 - val_acc: 0.6694\n",
      "Epoch 14/62\n",
      " - 0s - loss: 1.1559 - acc: 0.6149 - val_loss: 1.0602 - val_acc: 0.6694\n",
      "Epoch 15/62\n",
      " - 0s - loss: 1.1298 - acc: 0.6128 - val_loss: 1.0357 - val_acc: 0.6694\n",
      "Epoch 16/62\n",
      " - 0s - loss: 1.1090 - acc: 0.6211 - val_loss: 1.0189 - val_acc: 0.6860\n",
      "Epoch 17/62\n",
      " - 0s - loss: 1.0757 - acc: 0.6273 - val_loss: 1.0041 - val_acc: 0.6777\n",
      "Epoch 18/62\n",
      " - 0s - loss: 1.0466 - acc: 0.6315 - val_loss: 0.9745 - val_acc: 0.6612\n",
      "Epoch 19/62\n",
      " - 0s - loss: 1.0255 - acc: 0.6335 - val_loss: 0.9920 - val_acc: 0.7025\n",
      "Epoch 20/62\n",
      " - 0s - loss: 0.9913 - acc: 0.6480 - val_loss: 0.9456 - val_acc: 0.6777\n",
      "Epoch 21/62\n",
      " - 0s - loss: 0.9524 - acc: 0.6667 - val_loss: 0.9272 - val_acc: 0.6942\n",
      "Epoch 22/62\n",
      " - 0s - loss: 0.9212 - acc: 0.6791 - val_loss: 0.9113 - val_acc: 0.7025\n",
      "Epoch 23/62\n",
      " - 0s - loss: 0.8964 - acc: 0.6915 - val_loss: 0.8937 - val_acc: 0.7107\n",
      "Epoch 24/62\n",
      " - 0s - loss: 0.8734 - acc: 0.6894 - val_loss: 0.8831 - val_acc: 0.7190\n",
      "Epoch 25/62\n",
      " - 0s - loss: 0.8406 - acc: 0.6874 - val_loss: 0.8770 - val_acc: 0.7107\n",
      "Epoch 26/62\n",
      " - 0s - loss: 0.8136 - acc: 0.6957 - val_loss: 0.8592 - val_acc: 0.7107\n",
      "Epoch 27/62\n",
      " - 0s - loss: 0.7878 - acc: 0.6977 - val_loss: 0.8531 - val_acc: 0.7107\n",
      "Epoch 28/62\n",
      " - 0s - loss: 0.7661 - acc: 0.7039 - val_loss: 0.8429 - val_acc: 0.7107\n",
      "Epoch 29/62\n",
      " - 0s - loss: 0.7456 - acc: 0.7019 - val_loss: 0.8404 - val_acc: 0.7025\n",
      "Epoch 30/62\n",
      " - 0s - loss: 0.7234 - acc: 0.7081 - val_loss: 0.8271 - val_acc: 0.7107\n",
      "Epoch 31/62\n",
      " - 0s - loss: 0.7034 - acc: 0.7246 - val_loss: 0.8203 - val_acc: 0.7025\n",
      "Epoch 32/62\n",
      " - 0s - loss: 0.6827 - acc: 0.7184 - val_loss: 0.8178 - val_acc: 0.6860\n",
      "Epoch 33/62\n",
      " - 0s - loss: 0.6579 - acc: 0.7246 - val_loss: 0.8093 - val_acc: 0.6942\n",
      "Epoch 34/62\n",
      " - 0s - loss: 0.6399 - acc: 0.7308 - val_loss: 0.7969 - val_acc: 0.7025\n",
      "Epoch 35/62\n",
      " - 0s - loss: 0.6309 - acc: 0.7350 - val_loss: 0.7966 - val_acc: 0.7107\n",
      "Epoch 36/62\n",
      " - 0s - loss: 0.6094 - acc: 0.7453 - val_loss: 0.7991 - val_acc: 0.7025\n",
      "Epoch 37/62\n",
      " - 0s - loss: 0.5920 - acc: 0.7536 - val_loss: 0.7845 - val_acc: 0.7107\n",
      "Epoch 38/62\n",
      " - 0s - loss: 0.5783 - acc: 0.7557 - val_loss: 0.7755 - val_acc: 0.7025\n",
      "Epoch 39/62\n",
      " - 0s - loss: 0.5634 - acc: 0.7681 - val_loss: 0.7727 - val_acc: 0.7025\n",
      "Epoch 40/62\n",
      " - 0s - loss: 0.5543 - acc: 0.7702 - val_loss: 0.7786 - val_acc: 0.7273\n",
      "Epoch 41/62\n",
      " - 0s - loss: 0.5339 - acc: 0.7702 - val_loss: 0.7690 - val_acc: 0.7273\n",
      "Epoch 42/62\n",
      " - 0s - loss: 0.5204 - acc: 0.7992 - val_loss: 0.7671 - val_acc: 0.7438\n",
      "Epoch 43/62\n",
      " - 0s - loss: 0.5035 - acc: 0.7971 - val_loss: 0.7578 - val_acc: 0.7438\n",
      "Epoch 44/62\n",
      " - 0s - loss: 0.4959 - acc: 0.8012 - val_loss: 0.7558 - val_acc: 0.7355\n",
      "Epoch 45/62\n",
      " - 0s - loss: 0.4827 - acc: 0.8364 - val_loss: 0.7444 - val_acc: 0.7603\n",
      "Epoch 46/62\n",
      " - 0s - loss: 0.4649 - acc: 0.8199 - val_loss: 0.7446 - val_acc: 0.7769\n",
      "Epoch 47/62\n",
      " - 0s - loss: 0.4518 - acc: 0.8923 - val_loss: 0.7273 - val_acc: 0.7851\n",
      "Epoch 48/62\n",
      " - 0s - loss: 0.4403 - acc: 0.8344 - val_loss: 0.7296 - val_acc: 0.8099\n",
      "Epoch 49/62\n",
      " - 0s - loss: 0.4410 - acc: 0.9068 - val_loss: 0.7525 - val_acc: 0.7851\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=54, \n",
    "            epochs=62,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_test, y_test),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7851239797497583, loss: 0.7525272270864691\n"
     ]
    }
   ],
   "source": [
    "history = history.history\n",
    "print('Validation accuracy: {acc}, loss: {loss}'.format(acc=history['val_acc'][-1], loss=history['val_loss'][-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
